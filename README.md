# Tower Bloxx RL Agent ğŸ—ï¸ğŸ¤–

A reinforcement learning system that learns to play the mobile game **Tower Bloxx** by connecting to an Android device via USB/ADB.

## ğŸ® About the Game

**Tower Bloxx** is a physics-based block stacking game where:
- A crane swings left and right with a building block
- You tap to release the block at the right moment
- Goal: Stack blocks as precisely as possible to build the tallest tower
- Perfect placement earns bonus coins; misaligned blocks may cause the tower to collapse

## âœ¨ Features

- ğŸ“± **Real Device Training**: Connects to Android phone via ADB for authentic gameplay
- ğŸ§  **PPO Algorithm**: Uses Proximal Policy Optimization from Stable-Baselines3
- ğŸ¯ **CNN Policy**: Convolutional neural network for processing visual observations
- âš¡ **GPU Accelerated**: Optimized for NVIDIA RTX 3050 Ti (4GB VRAM)
- ğŸ“Š **TensorBoard Logging**: Real-time training metrics visualization
- ğŸ”„ **Frame Stacking**: 4-frame stack for motion perception
- ğŸ’¾ **Checkpointing**: Automatic model saving and best model tracking

## ğŸ› ï¸ Requirements

### Hardware
- **GPU**: NVIDIA RTX 3050 Ti (or compatible CUDA GPU)
- **RAM**: 16GB recommended
- **Android Device**: With USB debugging enabled

### Software
- Python 3.9+
- NVIDIA CUDA 12.6 (or compatible) `https://developer.nvidia.com/cuda-downloads`
- Android Debug Bridge (ADB)
- Tower Bloxx game installed on device

## ğŸ“¦ Installation

### 1. Clone and Setup Virtual Environment

```bash
cd d:\Opensource\towerblox-rl-agent

# Virtual environment already created, activate it:
.\venv\Scripts\activate
```

### 2. Install PyTorch with CUDA 12.6

```bash
pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu126
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

### 4. Verify Installation

```bash
python scripts/setup_environment.py
```

### 5. Test ADB Connection

```bash
# Connect your Android device via USB
adb devices

# Run ADB test
python scripts/test_adb_connection.py
```

## ğŸš€ Quick Start

### 1. Connect Device
1. Enable USB debugging on your Android device
2. Connect via USB cable
3. Approve USB debugging prompt on device
4. Open Tower Bloxx game

### 2. Test Environment

```bash
python scripts/test_environment.py
```

### 3. Start Training

```bash
# Train with default settings (2M timesteps)
python train.py

# Train for specific number of steps
python train.py --timesteps 500000

# Continue from checkpoint
python train.py --load ./logs/checkpoints/ppo_towerblox_50000_steps.zip
```

### 4. Monitor Training

```bash
# Open TensorBoard
tensorboard --logdir=./logs/tensorboard
```

### 5. Evaluate Trained Model

```bash
# Run evaluation
python evaluate.py --model ./logs/checkpoints/ppo_towerblox_final.zip -n 100

# Watch agent play
python evaluate.py --model ./logs/checkpoints/ppo_towerblox_final.zip --render
```

## ğŸ“ Project Structure

```
towerblox-rl-agent/
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ env_config.yaml      # Environment settings
â”‚   â”œâ”€â”€ ppo_config.yaml      # PPO hyperparameters
â”‚   â””â”€â”€ device_config.yaml   # ADB/device settings
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ environment/
â”‚   â”‚   â”œâ”€â”€ mobile_game_env.py   # Main Gymnasium environment
â”‚   â”‚   â”œâ”€â”€ screen_capture.py    # ADB screen capture
â”‚   â”‚   â”œâ”€â”€ action_executor.py   # Touch input execution
â”‚   â”‚   â””â”€â”€ reward_shaper.py     # Reward calculation
â”‚   â”œâ”€â”€ preprocessing/
â”‚   â”‚   â”œâ”€â”€ frame_processor.py   # Image preprocessing
â”‚   â”‚   â””â”€â”€ frame_stacker.py     # Frame stacking wrapper
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ trainer.py           # Main training loop
â”‚   â”‚   â””â”€â”€ callbacks.py         # SB3 callbacks
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ evaluator.py         # Model evaluation
â”‚   â”‚   â””â”€â”€ visualizer.py        # Visualization tools
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ adb_manager.py       # ADB connection handler
â”‚       â”œâ”€â”€ config_loader.py     # YAML config parser
â”‚       â”œâ”€â”€ logger.py            # Custom logging
â”‚       â””â”€â”€ gpu_monitor.py       # GPU monitoring
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup_environment.py     # Installation verification
â”‚   â”œâ”€â”€ test_adb_connection.py   # ADB connectivity test
â”‚   â””â”€â”€ test_environment.py      # Environment test
â”œâ”€â”€ train.py                     # Training entry point
â”œâ”€â”€ evaluate.py                  # Evaluation entry point
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ logs/                        # Training outputs
    â”œâ”€â”€ tensorboard/
    â”œâ”€â”€ checkpoints/
    â””â”€â”€ training_logs/
```

## âš™ï¸ Configuration

### Environment Config (`configs/env_config.yaml`)
- Observation settings (frame size, stacking)
- Action space definition
- Reward values
- Screen region coordinates

### PPO Config (`configs/ppo_config.yaml`)
- Learning rate: 3e-4
- Batch size: 64 (optimized for 4GB VRAM)
- n_steps: 2048
- n_epochs: 10
- gamma: 0.99

### Device Config (`configs/device_config.yaml`)
- Device serial number
- Screen dimensions
- ADB settings

## ğŸ§® Reward Structure

| Event | Reward |
|-------|--------|
| Perfect placement | +5.0 |
| Good placement | +2.0 |
| OK placement | +1.0 |
| Wobbly placement | +0.3 |
| Height bonus (per floor) | +0.5 |
| Coin collected | +1.0 |
| Step penalty | -0.01 |
| Game over | -10.0 |

## ğŸ¯ Action Space

| Action | Description |
|--------|-------------|
| 0 | Wait (do nothing) |
| 1 | Tap (release block) |

## ğŸ“Š Training Tips

1. **Start Small**: Begin with 100K steps to verify everything works
2. **Monitor GPU**: Watch VRAM usage via `nvidia-smi`
3. **Check TensorBoard**: Look for increasing reward trends
4. **Adjust Rewards**: If agent doesn't learn, modify reward values
5. **Frame Rate**: Target 30+ FPS for stable training

## ğŸ› Troubleshooting

### ADB Issues
```bash
# Restart ADB server
adb kill-server
adb start-server
adb devices
```

### CUDA Out of Memory
- Reduce batch_size in `ppo_config.yaml`
- Enable FP16 in config

### Slow Capture
- Check USB connection (use USB 3.0)
- Reduce screen capture resolution

## ğŸ“ˆ Expected Results

- **Random baseline**: ~-50 reward per episode
- **Trained agent (1M steps)**: ~+20-50 reward per episode
- **Expert agent (5M+ steps)**: ~+100+ reward per episode

## ğŸ“š References

- [Stable-Baselines3 Documentation](https://stable-baselines3.readthedocs.io/)
- [Gymnasium Documentation](https://gymnasium.farama.org/)
- [PPO Paper](https://arxiv.org/abs/1707.06347)

## ğŸ“ License

MIT License

---

**Built with â¤ï¸ using PyTorch, Stable-Baselines3, and Gymnasium**
